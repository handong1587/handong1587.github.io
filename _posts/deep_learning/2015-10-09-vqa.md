---
layout: post
category: deep_learning
title: Visual Question Answering
date: 2015-10-09
---

**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**

- intro: Facebook AI Research
- arxiv: [http://arxiv.org/abs/1502.05698v1](http://arxiv.org/abs/1502.05698v1)
- github: [https://github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)

**VQA: Visual Question Answering**

- intro: ICCV 2015
- arxiv: [http://arxiv.org/abs/1505.00468](http://arxiv.org/abs/1505.00468)
- homepage: [http://visualqa.org/](http://visualqa.org/)

**Ask Your Neurons: A Neural-based Approach to Answering Questions about Images**

- intro: ICCV 2015
- arxiv: [http://arxiv.org/abs/1505.01121](http://arxiv.org/abs/1505.01121)
- project: [https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/visual-turing-challenge/)
- video: [https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1](https://www.youtube.com/watch?v=QZEwDcN8ehs&hd=1)

**Exploring Models and Data for Image Question Answering**

![](https://camo.githubusercontent.com/ac498616bb6ea1db7aaabb1cf567d07e4bbef395/687474703a2f2f692e696d6775722e636f6d2f4a7669787832572e6a7067)

- arxiv: [http://arxiv.org/abs/1505.02074](http://arxiv.org/abs/1505.02074)
- gtihub(Tensorflow): [https://github.com/paarthneekhara/neural-vqa-tensorflow](https://github.com/paarthneekhara/neural-vqa-tensorflow)
- github(Python+Keras): [https://github.com/ayushoriginal/NeuralNetwork-ImageQA](https://github.com/ayushoriginal/NeuralNetwork-ImageQA)

**Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering**

- arxiv: [http://arxiv.org/abs/1505.05612](http://arxiv.org/abs/1505.05612)

**Teaching Machines to Read and Comprehend**

- intro: Google DeepMind
- arxiv: [http://arxiv.org/abs/1506.03340](http://arxiv.org/abs/1506.03340)
- github: [https://github.com/deepmind/rc-data](https://github.com/deepmind/rc-data)
- github(Theano/Blocks): [https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend](https://github.com/thomasmesnard/DeepMind-Teaching-Machines-to-Read-and-Comprehend)
- github(Tensorflow): [https://github.com/carpedm20/attentive-reader-tensorflow](https://github.com/carpedm20/attentive-reader-tensorflow)

**Neural Module Networks**

- intro: CVPR 2016
- arxiv: [http://arxiv.org/abs/1511.02799](http://arxiv.org/abs/1511.02799)
- github: [https://github.com/jacobandreas/nmn2](https://github.com/jacobandreas/nmn2)

**Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction**

![](http://cvlab.postech.ac.kr/research/dppnet/images/figure2.png)

- arxiv: [http://arxiv.org/abs/1511.05756](http://arxiv.org/abs/1511.05756)
- github: [https://github.com/HyeonwooNoh/DPPnet](https://github.com/HyeonwooNoh/DPPnet)
- project page: [http://cvlab.postech.ac.kr/research/dppnet/](http://cvlab.postech.ac.kr/research/dppnet/)

**Neural Generative Question Answering**

- arxiv: [http://arxiv.org/abs/1512.01337](http://arxiv.org/abs/1512.01337)

**Stacked Attention Networks for Image Question Answering**

- arxiv: [http://arxiv.org/abs/1511.02274](http://arxiv.org/abs/1511.02274)
- github: [https://github.com/abhshkdz/neural-vqa-attention](https://github.com/abhshkdz/neural-vqa-attention)

**Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering**

- arxiv: [http://arxiv.org/abs/1511.05234](http://arxiv.org/abs/1511.05234)

**Simple Baseline for Visual Question Answering**

- intro: Facebook AI Research. Bag-of-word
- arxiv: [http://arxiv.org/abs/1512.02167](http://arxiv.org/abs/1512.02167)
- github: [https://github.com/metalbubble/VQAbaseline](https://github.com/metalbubble/VQAbaseline)
- demo: [http://visualqa.csail.mit.edu/](http://visualqa.csail.mit.edu/)

**MovieQA: Understanding Stories in Movies through Question-Answering**

- intro: CVPR 2016
- project page: [http://movieqa.cs.toronto.edu/home/](http://movieqa.cs.toronto.edu/home/)
- arxiv: [http://arxiv.org/abs/1512.02902](http://arxiv.org/abs/1512.02902)
- gtihub: [https://github.com/makarandtapaswi/MovieQA_CVPR2016/](https://github.com/makarandtapaswi/MovieQA_CVPR2016/)

**Deeper LSTM+ normalized CNN for Visual Question Answering**

- intro: "This current code can get 58.16 on Open-Ended and 63.09 on Multiple-Choice on test-standard split"
- github: [https://github.com/VT-vision-lab/VQA_LSTM_CNN](https://github.com/VT-vision-lab/VQA_LSTM_CNN)

**A Neural Network for Factoid Question Answering over Paragraphs**

- project page: [http://cs.umd.edu/~miyyer/qblearn/](http://cs.umd.edu/~miyyer/qblearn/)
- paper: [https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf)
- code+data: [https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz](https://cs.umd.edu/~miyyer/qblearn/qanta.tar.gz)

**Learning to Compose Neural Networks for Question Answering**

- intro: NAACL 2016 Best paper
- arxiv: [http://arxiv.org/abs/1601.01705](http://arxiv.org/abs/1601.01705)

**Generating Natural Questions About an Image**

- arxiv: [http://arxiv.org/abs/1603.06059](http://arxiv.org/abs/1603.06059)

**Question Answering on Freebase via Relation Extraction and Textual Evidence**

- intro: ACL 2016
- arxiv: [https://arxiv.org/abs/1603.00957](https://arxiv.org/abs/1603.00957)
- github: [https://github.com/syxu828/QuestionAnsweringOverFB](https://github.com/syxu828/QuestionAnsweringOverFB)

**Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus**

- arxiv: [http://arxiv.org/abs/1603.06807](http://arxiv.org/abs/1603.06807)

**Character-Level Question Answering with Attention**

- arxiv: [http://arxiv.org/abs/1604.00727](http://arxiv.org/abs/1604.00727)
- comment(by @Wenpeng_Yin): "fancy model with minor improvement"

**A Focused Dynamic Attention Model for Visual Question Answering**

- arxiv: [http://arxiv.org/abs/1604.01485](http://arxiv.org/abs/1604.01485)

**Visual Question Answering Literature Survey**

- blog: [http://iamaaditya.github.io/research/literature/](http://iamaaditya.github.io/research/literature/)

**The DIY Guide to Visual Question Answering**

![](https://camo.githubusercontent.com/53c28e13bd645acbf49c9e71e82a36202d1981bc/687474703a2f2f7333322e706f7374696d672e6f72672f77636a6c7a7a7532742f53637265656e5f53686f745f323031365f30355f30385f61745f325f34325f30375f504d2e706e67)

- github: [https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md](https://github.com/jxieeducation/DIY-Data-Science/blob/master/research/visual_qa.md)

**Question Answering via Integer Programming over Semi-Structured Knowledge**

- arxiv: [http://arxiv.org/abs/1604.06076](http://arxiv.org/abs/1604.06076)
- github: [https://github.com/allenai/tableilp](https://github.com/allenai/tableilp)
- youtube: [https://www.youtube.com/watch?v=7NS53icQRrs](https://www.youtube.com/watch?v=7NS53icQRrs)

**Hierarchical Question-Image Co-Attention for Visual Question Answering**

- arxiv: [http://arxiv.org/abs/1606.00061](http://arxiv.org/abs/1606.00061)
- github: [https://github.com/jiasenlu/HieCoAttenVQA](https://github.com/jiasenlu/HieCoAttenVQA)

**Multimodal Residual Learning for Visual QA**

- arxiv: [http://arxiv.org/abs/1606.01455](http://arxiv.org/abs/1606.01455)

**Simple Question Answering by Attentive Convolutional Neural Network**

- arxiv: [http://arxiv.org/abs/1606.03391](http://arxiv.org/abs/1606.03391)

**Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?**

![](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/img/att_comparison_2row.jpg)

- homepage: [https://computing.ece.vt.edu/~abhshkdz/vqa-hat/](https://computing.ece.vt.edu/~abhshkdz/vqa-hat/)
- arxiv: [http://arxiv.org/abs/1606.03556](http://arxiv.org/abs/1606.03556)

**Simple and Effective Question Answering with Recurrent Neural Networks**

- arxiv: [http://arxiv.org/abs/1606.05029](http://arxiv.org/abs/1606.05029)

**Analyzing the Behavior of Visual Question Answering Models**

- arxiv: [http://arxiv.org/abs/1606.07356](http://arxiv.org/abs/1606.07356)

**Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding**

- arxiv: [https://arxiv.org/abs/1606.01847](https://arxiv.org/abs/1606.01847)
- github: [https://github.com/akirafukui/vqa-mcb](https://github.com/akirafukui/vqa-mcb)

**Deep Language Modeling for Question Answering using Keras**

- blog: [http://benjaminbolte.com/blog/2016/keras-language-modeling.html](http://benjaminbolte.com/blog/2016/keras-language-modeling.html)
- github: [https://github.com/codekansas/keras-language-modeling](https://github.com/codekansas/keras-language-modeling)

**Interpreting Visual Question Answering Models**

- arxiv: [http://arxiv.org/abs/1608.08974](http://arxiv.org/abs/1608.08974)

**The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering**

- intro: FSVQA
- arxiv: [http://arxiv.org/abs/1609.06657](http://arxiv.org/abs/1609.06657)

**Tutorial on Answering Questions about Images with Deep Learning**

- intro: The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016
- arxiv: [https://arxiv.org/abs/1610.01076](https://arxiv.org/abs/1610.01076)

**Hadamard Product for Low-rank Bilinear Pooling**

- arxiv: [https://arxiv.org/abs/1610.04325](https://arxiv.org/abs/1610.04325)
- github: [https://github.com/jnhwkim/MulLowBiVQA](https://github.com/jnhwkim/MulLowBiVQA)

**Open-Ended Visual Question-Answering**

![](https://raw.githubusercontent.com/imatge-upc/vqa-2016-cvprw/gh-pages/img/model.jpg)

- intro: Bachelor thesis report graded with A with honours at ETSETB Telecom BCN school, Universitat Polit\`ecnica de Catalunya (UPC). June 2016
- project page: [http://imatge-upc.github.io/vqa-2016-cvprw/](http://imatge-upc.github.io/vqa-2016-cvprw/)
- arxiv: [https://arxiv.org/abs/1610.02692](https://arxiv.org/abs/1610.02692)
- slides: [http://www.slideshare.net/xavigiro/openended-visual-questionanswering](http://www.slideshare.net/xavigiro/openended-visual-questionanswering)
- github: [https://github.com/imatge-upc/vqa-2016-cvprw](https://github.com/imatge-upc/vqa-2016-cvprw)

**Deep Learning for Question Answering**

- intro: UMD. Mohit Iyyer.
- intro: Recurrent Neural Networks, Recursive Neural Network
- slides: [http://cs.umd.edu/~miyyer/data/deepqa.pdf](http://cs.umd.edu/~miyyer/data/deepqa.pdf)

**Dual Attention Networks for Multimodal Reasoning and Matching**

- arxiv: [https://arxiv.org/abs/1611.00471](https://arxiv.org/abs/1611.00471)

**Leveraging Video Descriptions to Learn Video Question Answering**

- intro: AAAI 2017
- arxiv: [https://arxiv.org/abs/1611.04021](https://arxiv.org/abs/1611.04021)

**Dynamic Coattention Networks For Question Answering**

- arxiv: [https://arxiv.org/abs/1611.01604](https://arxiv.org/abs/1611.01604)

**State of the art deep learning model for question answering**

- blog: [http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/](http://metamind.io/research/state-of-the-art-deep-learning-model-for-question-answering/)

**Zero-Shot Visual Question Answering**

- arxiv: [https://arxiv.org/abs/1611.05546](https://arxiv.org/abs/1611.05546)

**Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation**

- intro: University of Rochester & Microsoft & University College London
- arxiv: [https://arxiv.org/abs/1701.08251](https://arxiv.org/abs/1701.08251)

**Question Answering through Transfer Learning from Large Fine-grained Supervision Data**

- intro: Seoul National University & University of Washington
- arxiv: [https://arxiv.org/abs/1702.02171](https://arxiv.org/abs/1702.02171)

**Question Answering from Unstructured Text by Retrieval and Comprehension**

- arxiv: [https://arxiv.org/abs/1703.08885](https://arxiv.org/abs/1703.08885)
- notes: [https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/](https://theneuralperspective.com/2017/04/26/question-answering-from-unstructured-text-by-retrieval-and-comprehension/)

**Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering**

- intro: Google Research
- arxiv: [https://arxiv.org/abs/1704.03162](https://arxiv.org/abs/1704.03162)

**Learning to Reason: End-to-End Module Networks for Visual Question Answering**

- intro: UC Berkeley, Boston University
- arxiv: [https://arxiv.org/abs/1704.05526](https://arxiv.org/abs/1704.05526)

**TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering**

- intro: CVPR 2017.Seoul National University & Yahoo Research
- arxiv: [https://arxiv.org/abs/1704.04497](https://arxiv.org/abs/1704.04497)
- github: [https://github.com/YunseokJANG/tgif-qa](https://github.com/YunseokJANG/tgif-qa)

**Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks**

- intro: ACL 2017 (short)
- project page: [https://rajarshd.github.io/TextKBQA/](https://rajarshd.github.io/TextKBQA/)
- arxiv: [https://arxiv.org/abs/1704.08384](https://arxiv.org/abs/1704.08384)
- github: [https://github.com/rajarshd/TextKBQA](https://github.com/rajarshd/TextKBQA)

**Learning Convolutional Text Representations for Visual Question Answering**

- arxiv: [https://arxiv.org/abs/1705.06824](https://arxiv.org/abs/1705.06824)
- github: [https://github.com/divelab/vqa-text](https://github.com/divelab/vqa-text)

**Compact Tensor Pooling for Visual Question Answering**

[https://arxiv.org/abs/1706.06706](https://arxiv.org/abs/1706.06706)

**DeepStory: Video Story QA by Deep Embedded Memory Networks**

- intro: IJCAI 2017. Seoul National University
- arxiv: [https://arxiv.org/abs/1707.00836](https://arxiv.org/abs/1707.00836)

**Long-Term Memory Networks for Question Answering**

- intro: SUNY Buffalo & LinkedIn & LinkedIn
- arxiv: [https://arxiv.org/abs/1707.01961](https://arxiv.org/abs/1707.01961)

**Video Question Answering via Attribute-Augmented Attention Network Learning**

- intro: SIGIR 2017
- arxiv: [https://arxiv.org/abs/1707.06355](https://arxiv.org/abs/1707.06355)

**Bottom-Up and Top-Down Attention for Image Captioning and VQA**

[https://arxiv.org/abs/1707.07998](https://arxiv.org/abs/1707.07998)

**Structured Attentions for Visual Question Answering**

- intro: ICCV 2017
- arxiv: [https://arxiv.org/abs/1708.02071](https://arxiv.org/abs/1708.02071)
- github: [https://github.com/zhuchen03/vqa-sva](https://github.com/zhuchen03/vqa-sva)

**Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge**

- intro: Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR
- intro: The University of Adelaide & Australian National University & Microsoft Research
- arxiv: [https://arxiv.org/abs/1708.02711](https://arxiv.org/abs/1708.02711)

**MemexQA: Visual Memex Question Answering**

- intro: Carnegie Mellon University, Customer Service AI, Yahoo
- project page: [https://memexqa.cs.cmu.edu/](https://memexqa.cs.cmu.edu/)
- arxiv: [https://arxiv.org/abs/1708.01336](https://arxiv.org/abs/1708.01336)

**Automatic Question-Answering Using A Deep Similarity Neural Network**

- intro: New York University & AT&T Research Labs
- arxiv: [https://arxiv.org/abs/1708.01713](https://arxiv.org/abs/1708.01713)

**Question Dependent Recurrent Entity Network for Question Answering**

- intro: University of Pisa
- arxiv: [https://arxiv.org/abs/1707.07922](https://arxiv.org/abs/1707.07922)
- github: [https://github.com/andreamad8/QDREN](https://github.com/andreamad8/QDREN)

# Projects

**VQA Demo: Visual Question Answering Demo on pretrained model**

- github: [https://github.com/iamaaditya/VQA_Demo](https://github.com/iamaaditya/VQA_Demo)
- ref: [http://iamaaditya.github.io/research/](http://iamaaditya.github.io/research/)

**deep-qa: Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task**

- github: [https://github.com/aseveryn/deep-qa](https://github.com/aseveryn/deep-qa)

**YodaQA: A Question Answering system built on top of the Apache UIMA framework**

- homepage: [http://ailao.eu/yodaqa/](http://ailao.eu/yodaqa/)
- github: [https://github.com/brmson/yodaqa](https://github.com/brmson/yodaqa)

**insuranceQA-cnn-lstm: tensorflow and theano cnn code for insurance QA(question Answer matching)**

- github: [https://github.com/white127/insuranceQA-cnn-lstm](https://github.com/white127/insuranceQA-cnn-lstm)

**Tensorflow Implementation of Deeper LSTM+ normalized CNN for Visual Question Answering**

![](https://cloud.githubusercontent.com/assets/19935904/16358326/e6812310-3add-11e6-914f-c61c19d6ab5a.png)

- github: [https://github.com/JamesChuanggg/VQA-tensorflow](https://github.com/JamesChuanggg/VQA-tensorflow)

**Visual Question Answering with Keras**

![](https://camo.githubusercontent.com/f52d44199710c8f3939fb182a339d1d6a0b09a3f/687474703a2f2f692e696d6775722e636f6d2f327a4a30396d512e706e67)

- project page: [https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/](https://anantzoid.github.io/VQA-Keras-Visual-Question-Answering/)
- github: [https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering](https://github.com/anantzoid/VQA-Keras-Visual-Question-Answering)

**Deep Learning Models for Question Answering with Keras**

- blog: [http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html](http://sujitpal.blogspot.jp/2016/10/deep-learning-models-for-question.html)

**GuessWhat?! Visual object discovery through multi-modal dialogue**

- intro: University of Montreal & Univ. Lille & Google DeepMind & Twitter
- arxiv: [https://arxiv.org/abs/1611.08481](https://arxiv.org/abs/1611.08481)

**Deep QA: Using deep learning to answer Aristo's science questions**

- github: [https://github.com/allenai/deep_qa](https://github.com/allenai/deep_qa)

**Visual Question Answering in Pytorch**

[https://github.com/Cadene/vqa.pytorch](https://github.com/Cadene/vqa.pytorch)

# Dataset

**Visual7W: Grounded Question Answering in Images**

![](http://web.stanford.edu/~yukez/images/img/visual7w_examples.png)

- homepage: [http://web.stanford.edu/~yukez/visual7w/](http://web.stanford.edu/~yukez/visual7w/)
- github: [https://github.com/yukezhu/visual7w-toolkit](https://github.com/yukezhu/visual7w-toolkit)
- github: [https://github.com/yukezhu/visual7w-qa-models](https://github.com/yukezhu/visual7w-qa-models)

# Resources

**Awesome Visual Question Answering**

- github: [https://github.com/JamesChuanggg/awesome-vqa](https://github.com/JamesChuanggg/awesome-vqa)